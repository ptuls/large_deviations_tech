\section{Background and Motivation}

As mentioned in the introduction, the study of random processes are central in network algorithmics. 
Common mathematical tools to bound the worst-case behavior includes the Markov's inequality,
Chebyshev's inequality, the Chernoff bound and 
Azuma-Hoeffding inequality \cite{Azuma67Martingale,Hoeffding63Bounded}. Overviews of these
tools (and many others) can be found in \cite{Alon04ProbMethod,MitzenmacherProb05,Raghavan95RandAlgo}.

Why do we care about finding strong, refined worst-case probability bounds? Or equivalently, can one just
make do with coarse bound?

The question can only be answered in context: if the problem can tolerate a wide margin of
``bad'' events, then a refined worst-case probability bound is unnecessary. More often than not, however,
due to the high-speed, large data volume environment, stronger bounds are a necessity.

Consider the issue of packets arriving out-of-order after traversing a switch. TCP-based applications
are sensitive to this packet reordering. A TCP-based application will drop the set of packets it has received thus far, and
issue a retransmission to the sender, causing wastage of bandwidth. Suppose as a designer of a switch, at 
worst, we know that packets will arrive out-of-order with probability $1/1000$. Then, roughly 1 out of a 
1000 packets will be out-of-order. Suppose each packet is has size 100 bytes, then on a 100 Gbps link,
it takes 8 ns to transmit a single packet. Then, we expect about 1 packet to be out-of-order every 8 
$\mu$s. This may not be acceptable for some applications, so stronger bounds are required.

The lesson here is that the volume of data processed by solutions from network algorithmics are massive,
and so are large enough for rare events to be seen in any given sample set. 

We summarize a few areas where strong bounds are required:

\medskip
\noindent
\textbf{Distributed data streaming problems}:  Analyzing massive aggregate traffic streams through many high-speed nodes (or links) for 
detecting global events (\eg global elephants) that can spread themselves even and thin over these nodes (to avoid detection) or for estimating 
global statistics in a communication-efficient manner, known as {\it distributed data streaming} (DDS),  has
emerged in recent years as an important research topic in network monitoring.
In later sections, we shall see that worst-case large deviation issues arise naturally in the design and analysis
of DDS algorithms, where we need to obtain a tight
probability tail bound on its detection or estimation accuracy no matter how the global event or statistic is split into
fragments of different sizes across these nodes. The design of such stochastically robust solutions for 
two important DDS problems remain open: estimation of the empirical entropy of and association rule mining over distributed data streams.

\medskip
\noindent
\textbf{Large per-flow data structure problems}:
During a measurement period, tens of millions of application flows may pass through an Internet gateway. Many network algorithmics methods have been developed that make use of per-flow state information, \eg per-flow scheduling, stateful firewalls, and multi-packet deep packet inspection for intrusion and virus detection. However, it is very challenging to design massive flow-level data structures that are capable of providing extremely high throughput and low bounded access delay under arbitrary workloads, yet have relatively low costs. Randomized 
algorithmic solutions that can provide strong worst-case stochastic performance guarantees while using commodity memories
require strong stochastic bounds on their behavior (see for instance \cite{Hua08BRICK,Zhao09DRAM}).

\medskip
\noindent
\textbf{Load-balanced switching problems}:
Load-balanced routers are an important class of switch architectures that
provides scalability and throughput guarantees.  However, the plain vanilla 
load-balanced routers do not preserve the order of packets even within a TCP or UDP flow,
which could cause performance problems for many network applications~\cite{Chang02BvN1,Chang02BvN2}.
While forcing all packets within a TCP flow go down the same path (\eg determined by hashing on the flow identifier of each packet)
inside a router will eliminate this packet reordering issue, we will show that the significant heterogeneity in flow rates and sizes
seen in today's Internet will leave 
these routers severely load-imbalanced, defeating the very purpose for which they were designed. A randomized solution could allow, with
a small probability of error, smaller buffers for the reordering for packets in a flow.


In the rest of the paper, we
discuss some useful stochastic orderings and properties of random variables that help with the development
of tighter worst-case bounds. 

